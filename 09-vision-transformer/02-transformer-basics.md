# Transformer ì•„í‚¤í…ì²˜

> Encoder-Decoder êµ¬ì¡°ì˜ ì´í•´

## ê°œìš”

ì•ì„œ [ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜](./01-attention-mechanism.md)ì—ì„œ Self-Attentionê³¼ Multi-Head Attentionì˜ ì›ë¦¬ë¥¼ ë°°ì› ìŠµë‹ˆë‹¤. ì´ì œ ì´ ë¶€í’ˆë“¤ì´ ì–´ë–»ê²Œ ì¡°ë¦½ë˜ì–´ í•˜ë‚˜ì˜ ì™„ì „í•œ ì•„í‚¤í…ì²˜ê°€ ë˜ëŠ”ì§€ ì‚´í´ë³¼ ì°¨ë¡€ì…ë‹ˆë‹¤. ì´ ì„¹ì…˜ì—ì„œëŠ” **Transformer**ì˜ ì „ì²´ êµ¬ì¡° â€” Encoder, Decoder, Positional Encoding, Feed-Forward Network â€” ë¥¼ ë¹ ì§ì—†ì´ ë‹¤ë£¹ë‹ˆë‹¤.

**ì„ ìˆ˜ ì§€ì‹**: [ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜](./01-attention-mechanism.md)ì˜ Scaled Dot-Product Attentionê³¼ Multi-Head Attention
**í•™ìŠµ ëª©í‘œ**:
- Transformerì˜ Encoder-Decoder êµ¬ì¡°ë¥¼ ì „ì²´ì ìœ¼ë¡œ íŒŒì•…í•˜ê¸°
- Positional Encodingì´ ì™œ í•„ìš”í•˜ê³  ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì´í•´í•˜ê¸°
- Feed-Forward Network, Residual Connection, Layer Normì˜ ì—­í•  ì•Œê¸°
- Encoder-Only, Decoder-Only ë“± í˜„ëŒ€ ë³€í˜•ì„ êµ¬ë¶„í•˜ê¸°

## ì™œ ì•Œì•„ì•¼ í• ê¹Œ?

TransformerëŠ” 2017ë…„ì— ë“±ì¥í•œ ì´í›„, í˜„ëŒ€ AIì˜ **ì‚¬ì‹¤ìƒ í‘œì¤€ ì•„í‚¤í…ì²˜**ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. GPT, BERT, ViT, DALL-E, Stable Diffusion, Sora â€” ì´ ëª¨ë“  ëª¨ë¸ì˜ ë¼ˆëŒ€ê°€ Transformerì´ì£ . ì»´í“¨í„° ë¹„ì „ì—ì„œë„ [Vision Transformer](./03-vit.md)ê°€ CNNì„ ëŒ€ì²´í•˜ê¸° ì‹œì‘í•˜ë©´ì„œ, Transformer ì•„í‚¤í…ì²˜ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì€ ì„ íƒì´ ì•„ë‹Œ **í•„ìˆ˜**ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.

Transformerë¥¼ ì´í•´í•˜ë©´ ì´í›„ì— ë°°ìš¸ ViT, Swin Transformer, DETR ê°™ì€ ë¹„ì „ ëª¨ë¸ì´ ì™œ ê·¸ëŸ° êµ¬ì¡°ë¥¼ ê°€ì§€ëŠ”ì§€ ìì—°ìŠ¤ëŸ½ê²Œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## í•µì‹¬ ê°œë…

### ê°œë… 1: ì „ì²´ êµ¬ì¡° â€” "í†µì—­ì‚¬ ë‘ ëª…ì˜ í˜‘ì—…"

> ğŸ’¡ **ë¹„ìœ **: í•œêµ­ì–´ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•˜ëŠ” í†µì—­ íŒ€ì„ ìƒìƒí•´ ë³´ì„¸ìš”. **ì¸ì½”ë”(Encoder)**ëŠ” í•œêµ­ì–´ ë¬¸ì¥ì„ ë“£ê³  "ì˜ë¯¸"ë¥¼ ë¨¸ë¦¿ì†ì— ì •ë¦¬í•˜ëŠ” ì²« ë²ˆì§¸ í†µì—­ì‚¬ì…ë‹ˆë‹¤. **ë””ì½”ë”(Decoder)**ëŠ” ê·¸ ì •ë¦¬ëœ ì˜ë¯¸ë¥¼ ë°›ì•„ì„œ ì˜ì–´ ë¬¸ì¥ìœ¼ë¡œ í•œ ë‹¨ì–´ì”© í’€ì–´ë‚´ëŠ” ë‘ ë²ˆì§¸ í†µì—­ì‚¬ì´ê³ ìš”. ë‘˜ ì‚¬ì´ë¥¼ ì—°ê²°í•˜ëŠ” ê²ƒì´ **Cross-Attention**ì…ë‹ˆë‹¤.

Transformerì˜ ì „ì²´ êµ¬ì¡°ë¥¼ í•œëˆˆì— ì •ë¦¬í•˜ë©´ ì´ë ‡ìŠµë‹ˆë‹¤:

**Encoder (ì¸ì½”ë”)**
- ì…ë ¥ â†’ ì„ë² ë”© + Positional Encoding
- Nê°œì˜ ì¸ì½”ë” ë ˆì´ì–´ ë°˜ë³µ:
  - Multi-Head Self-Attention
  - Add & Norm (ì”ì°¨ ì—°ê²° + ì •ê·œí™”)
  - Feed-Forward Network
  - Add & Norm

**Decoder (ë””ì½”ë”)**
- ì¶œë ¥(ì‹œí”„íŠ¸) â†’ ì„ë² ë”© + Positional Encoding
- Nê°œì˜ ë””ì½”ë” ë ˆì´ì–´ ë°˜ë³µ:
  - Masked Multi-Head Self-Attention
  - Add & Norm
  - Multi-Head **Cross**-Attention (ì¸ì½”ë” ì¶œë ¥ ì°¸ì¡°!)
  - Add & Norm
  - Feed-Forward Network
  - Add & Norm

ì›ë˜ ë…¼ë¬¸ì—ì„œëŠ” ì¸ì½”ë”ì™€ ë””ì½”ë” ê°ê° **N=6ê°œ ë ˆì´ì–´**ë¥¼ ìŒ“ì•˜ìŠµë‹ˆë‹¤.

> âš ï¸ **í”í•œ ì˜¤í•´**: "TransformerëŠ” í•­ìƒ Encoder + Decoderë¥¼ í•¨ê»˜ ì‚¬ìš©í•œë‹¤"ê³  ìƒê°í•˜ê¸° ì‰½ì§€ë§Œ, ì‹¤ì œë¡œ í˜„ëŒ€ ëª¨ë¸ ëŒ€ë¶€ë¶„ì€ **í•œìª½ë§Œ** ì‚¬ìš©í•©ë‹ˆë‹¤. BERTëŠ” Encoderë§Œ, GPTëŠ” Decoderë§Œ ì‚¬ìš©í•˜ì£ . ì›ë˜ì˜ Encoder-Decoder êµ¬ì¡°ëŠ” ë²ˆì—­ì²˜ëŸ¼ ì…ë ¥ê³¼ ì¶œë ¥ í˜•íƒœê°€ ë‹¤ë¥¸ ì‘ì—…ì— ì í•©í•©ë‹ˆë‹¤.

### ê°œë… 2: Positional Encoding â€” "ì‹œê³„ì˜ ì‹œì¹¨, ë¶„ì¹¨, ì´ˆì¹¨"

Transformerì˜ Self-Attentionì€ ëª¨ë“  ìœ„ì¹˜ë¥¼ ë™ì‹œì— ë´…ë‹ˆë‹¤. ì´ê²ƒì€ ê°•ë ¥í•œ ì¥ì ì´ì§€ë§Œ, í•œ ê°€ì§€ ì¹˜ëª…ì ì¸ ë¬¸ì œê°€ ìˆì–´ìš”. **ìˆœì„œ ì •ë³´ê°€ ì‚¬ë¼ì§‘ë‹ˆë‹¤!**

"ê³ ì–‘ì´ê°€ ì¥ë¥¼ ì¡ì•˜ë‹¤"ì™€ "ì¥ê°€ ê³ ì–‘ì´ë¥¼ ì¡ì•˜ë‹¤"ê°€ ê°™ì€ ì…ë ¥ìœ¼ë¡œ ì²˜ë¦¬ë  ìˆ˜ ìˆë‹¤ëŠ” ëœ»ì´ì£ . RNNì€ ë‹¨ì–´ë¥¼ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬í•˜ë‹ˆê¹Œ ìì—°ìŠ¤ëŸ½ê²Œ ìˆœì„œê°€ ë³´ì¡´ë˜ì§€ë§Œ, TransformerëŠ” ëª¨ë“  ë‹¨ì–´ë¥¼ í•œ ë²ˆì— ë³´ê¸° ë•Œë¬¸ì— **ìœ„ì¹˜ ì •ë³´ë¥¼ ë³„ë„ë¡œ ì£¼ì…**í•´ì•¼ í•©ë‹ˆë‹¤.

> ğŸ’¡ **ë¹„ìœ **: ì‹œê³„ë¥¼ ìƒê°í•´ ë³´ì„¸ìš”. ì‹œì¹¨(ëŠë¦° ì£¼íŒŒìˆ˜)ì€ ëŒ€ëµì ì¸ ì‹œê°„ëŒ€ë¥¼, ë¶„ì¹¨(ì¤‘ê°„ ì£¼íŒŒìˆ˜)ì€ ë¶„ ë‹¨ìœ„ë¥¼, ì´ˆì¹¨(ë¹ ë¥¸ ì£¼íŒŒìˆ˜)ì€ ì´ˆ ë‹¨ìœ„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì„¸ ë°”ëŠ˜ì˜ ì¡°í•©ìœ¼ë¡œ í•˜ë£¨ ì¤‘ **ì–´ë–¤ ìˆœê°„ì´ë“  ê³ ìœ í•˜ê²Œ** í‘œí˜„í•  ìˆ˜ ìˆì£ . Positional Encodingë„ ë˜‘ê°™ì€ ì›ë¦¬ì…ë‹ˆë‹¤!

ì›ë˜ ë…¼ë¬¸ì—ì„œëŠ” ì‚¬ì¸(sine)ê³¼ ì½”ì‚¬ì¸(cosine) í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

- $pos$: ì‹œí€€ìŠ¤ì—ì„œì˜ ìœ„ì¹˜ (0, 1, 2, ...)
- $i$: ì°¨ì› ì¸ë±ìŠ¤
- $d_{model}$: ëª¨ë¸ì˜ ì°¨ì› ìˆ˜

í•µì‹¬ ì•„ì´ë””ì–´ëŠ” **ì„œë¡œ ë‹¤ë¥¸ ì£¼íŒŒìˆ˜ì˜ sin/cos íŒŒë™**ì„ ì¡°í•©í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤:

| ì°¨ì› | ì£¼íŒŒìˆ˜ | ì—­í•  |
|------|--------|------|
| ë‚®ì€ ì°¨ì› (i ì‘ìŒ) | ê³ ì£¼íŒŒ (ë¹ ë¥¸ ë³€í™”) | ì¸ì ‘ ìœ„ì¹˜ ì„¸ë°€í•˜ê²Œ êµ¬ë¶„ |
| ë†’ì€ ì°¨ì› (i í¼) | ì €ì£¼íŒŒ (ëŠë¦° ë³€í™”) | ë¨¼ ê±°ë¦¬ì˜ ìœ„ì¹˜ ê´€ê³„ í¬ì°© |

ì´ ë°©ì‹ì˜ ìˆ˜í•™ì ì¸ ì•„ë¦„ë‹¤ì›€ì€, ì„ì˜ì˜ ê³ ì • ì˜¤í”„ì…‹ $k$ì— ëŒ€í•´ $PE(pos+k)$ê°€ $PE(pos)$ì˜ **ì„ í˜• ë³€í™˜**ìœ¼ë¡œ í‘œí˜„ëœë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ë•ë¶„ì— ëª¨ë¸ì´ "3ì¹¸ ë–¨ì–´ì§„ ìœ„ì¹˜"ê°™ì€ **ìƒëŒ€ì  ìœ„ì¹˜**ë¥¼ ì‰½ê²Œ í•™ìŠµí•  ìˆ˜ ìˆì–´ìš”.

### ê°œë… 3: Feed-Forward Network â€” "ì •ë³´ ê°€ê³µ ê³µì¥"

> ğŸ’¡ **ë¹„ìœ **: Self-Attentionì´ "íšŒì˜ì‹¤ì—ì„œ ëª¨ë“  ì‚¬ëŒì˜ ì˜ê²¬ì„ ë“£ëŠ” ê³¼ì •"ì´ë¼ë©´, Feed-Forward Network(FFN)ì€ "íšŒì˜ ë‚´ìš©ì„ ì •ë¦¬í•´ì„œ ê²°ë¡ ì„ ë‚´ë¦¬ëŠ” ê³¼ì •"ì…ë‹ˆë‹¤. ì–´í…ì…˜ì€ **ì–´ë””ì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ì§€** ê²°ì •í•˜ê³ , FFNì€ ê°€ì ¸ì˜¨ ì •ë³´ë¡œ **ë¬´ì—‡ì„ í• ì§€** ê²°ì •í•©ë‹ˆë‹¤.

FFNì˜ êµ¬ì¡°ëŠ” ì˜ì™¸ë¡œ ë‹¨ìˆœí•©ë‹ˆë‹¤. ë‘ ê°œì˜ ì„ í˜• ë³€í™˜ ì‚¬ì´ì— í™œì„±í™” í•¨ìˆ˜ë¥¼ ë¼ìš´ ê²ƒì´ ì „ë¶€ì˜ˆìš”:

$$\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2$$

- $W_1$: $(d_{model}, d_{ff})$ â€” ì°¨ì›ì„ í™•ì¥ (ë³´í†µ $d_{ff} = 4 \times d_{model}$)
- $W_2$: $(d_{ff}, d_{model})$ â€” ë‹¤ì‹œ ì›ë˜ ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ

ì™œ 4ë°°ë¡œ í™•ì¥í–ˆë‹¤ê°€ ë‹¤ì‹œ ì¤„ì¼ê¹Œìš”? ì´ê²ƒì€ **ì •ë³´ ë³‘ëª©(information bottleneck)** íŒ¨í„´ì…ë‹ˆë‹¤. í•œ ë²ˆ ë„“ì€ ê³µê°„ì—ì„œ ë¹„ì„ í˜• ë³€í™˜ì„ ê±°ì¹œ í›„ ë‹¤ì‹œ ì••ì¶•í•˜ë©´, ë” í’ë¶€í•œ í‘œí˜„ì´ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤.

> ğŸ’¡ **ì•Œê³  ê³„ì…¨ë‚˜ìš”?**: ìµœê·¼ ì—°êµ¬ì—ì„œ FFNì€ ì‚¬ì‹¤ **ê±°ëŒ€í•œ ë©”ëª¨ë¦¬**ì²˜ëŸ¼ ì‘ë™í•œë‹¤ëŠ” ê²ƒì´ ë°í˜€ì¡ŒìŠµë‹ˆë‹¤. FFNì˜ Key-Value êµ¬ì¡°ê°€ ì‚¬ì‹¤ìƒ í•™ìŠµëœ "ì‚¬ì‹¤(fact)"ì„ ì €ì¥í•˜ëŠ” ì—­í• ì„ í•˜ëŠ” ê²ƒì´ì£ . "íŒŒë¦¬ëŠ” í”„ë‘ìŠ¤ì˜ ìˆ˜ë„ë‹¤" ê°™ì€ ì§€ì‹ì´ FFNì— ì €ì¥ëœë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.

### ê°œë… 4: Residual Connectionê³¼ Layer Norm â€” "ì•ˆì „ë§ê³¼ ì˜¨ë„ ì¡°ì ˆ"

TransformerëŠ” ë ˆì´ì–´ë¥¼ ê¹Šê²Œ ìŒ“ìŠµë‹ˆë‹¤. ê¹Šì–´ì§ˆìˆ˜ë¡ **ê¸°ìš¸ê¸° ì†Œì‹¤(vanishing gradient)** ë¬¸ì œê°€ ì‹¬í•´ì§€ëŠ”ë°, ì´ë¥¼ í•´ê²°í•˜ëŠ” ë‘ ê°€ì§€ í•µì‹¬ ì¥ì¹˜ê°€ ìˆìŠµë‹ˆë‹¤.

**Residual Connection (ì”ì°¨ ì—°ê²°)**

> ğŸ’¡ **ë¹„ìœ **: ê³ ì†ë„ë¡œì˜ ì§€ë¦„ê¸¸(bypass)ì´ì—ìš”. ë³µì¡í•œ ì‹œë‚´ ë„ë¡œ(ë ˆì´ì–´)ë¥¼ ê±°ì¹˜ì§€ ì•Šê³  ëª©ì ì§€ë¡œ ë°”ë¡œ ê°ˆ ìˆ˜ ìˆëŠ” ê¸¸ì„ ë§Œë“¤ì–´ ë‘” ê²ë‹ˆë‹¤. [ResNet](../05-cnn-architectures/03-resnet.md)ì—ì„œ ë°°ìš´ Skip Connectionê³¼ ì™„ì „íˆ ê°™ì€ ê°œë…ì´ì—ìš”!

$$\text{output} = x + \text{SubLayer}(x)$$

ë ˆì´ì–´ê°€ ì•„ë¬´ê²ƒë„ í•™ìŠµí•˜ì§€ ëª»í•˜ë”ë¼ë„ ìµœì†Œí•œ ì…ë ¥ì„ ê·¸ëŒ€ë¡œ í†µê³¼ì‹œí‚¬ ìˆ˜ ìˆìœ¼ë‹ˆ, ê¹Šì€ ë„¤íŠ¸ì›Œí¬ë„ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµë©ë‹ˆë‹¤.

**Layer Normalization**

[ë°°ì¹˜ ì •ê·œí™”](../04-cnn-fundamentals/03-batch-normalization.md)ì—ì„œ BatchNormì„ ë°°ì› ì£ ? Transformerì—ì„œëŠ” **Layer Norm**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë°°ì¹˜ ì°¨ì›ì´ ì•„ë‹ˆë¼ **íŠ¹ì„± ì°¨ì›**ì„ ë”°ë¼ ì •ê·œí™”í•˜ê¸° ë•Œë¬¸ì—, ë°°ì¹˜ í¬ê¸°ì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.

$$\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$$

ì´ ë‘ ì¥ì¹˜ë¥¼ ê²°í•©í•œ **"Add & Norm"** íŒ¨í„´ì´ Transformer ê³³ê³³ì— ë°˜ë³µë©ë‹ˆë‹¤.

**Post-Norm vs Pre-Norm â€” í˜„ëŒ€ì˜ ì„ íƒ**

ì›ë˜ ë…¼ë¬¸ì€ **Post-Norm** ë°©ì‹ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤:

> Post-Norm: $\text{LayerNorm}(x + \text{SubLayer}(x))$ â€” ì„œë¸Œë ˆì´ì–´ ì´í›„ ì •ê·œí™”

í•˜ì§€ë§Œ í˜„ëŒ€ ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸(GPT-3, LLaMA, PaLM ë“±)ì€ **Pre-Norm**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

> Pre-Norm: $x + \text{SubLayer}(\text{LayerNorm}(x))$ â€” ì„œë¸Œë ˆì´ì–´ ì´ì „ì— ì •ê·œí™”

Pre-Normì´ í•™ìŠµ ì´ˆê¸° ì•ˆì •ì„±ì´ í›¨ì”¬ ì¢‹ì•„ì„œ, ìˆ˜ì‹­~ìˆ˜ë°± ë ˆì´ì–´ë¥¼ ìŒ“ëŠ” ëŒ€ê·œëª¨ ëª¨ë¸ì—ì„œëŠ” ê±°ì˜ í•„ìˆ˜ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.

### ê°œë… 5: í˜„ëŒ€ Transformer ë³€í˜•ë“¤

ì›ë˜ì˜ Encoder-Decoder êµ¬ì¡°ì—ì„œ ë°œì „í•œ ì„¸ ê°€ì§€ ì£¼ìš” ë³€í˜•ì´ ìˆìŠµë‹ˆë‹¤:

| êµ¬ì¡° | ëŒ€í‘œ ëª¨ë¸ | ìš©ë„ | ë¹„ì „ ì ìš© |
|------|----------|------|----------|
| **Encoder-Only** | BERT, ViT | ë¶„ë¥˜, ì„ë² ë”© | ì´ë¯¸ì§€ ë¶„ë¥˜, íŠ¹ì§• ì¶”ì¶œ |
| **Decoder-Only** | GPT, LLaMA | ìƒì„± | ì´ë¯¸ì§€ ìƒì„± (ì¼ë¶€) |
| **Encoder-Decoder** | T5, BART | ë²ˆì—­, ìš”ì•½ | ìº¡ì…”ë‹, VQA |

ì»´í“¨í„° ë¹„ì „ì—ì„œëŠ” ì£¼ë¡œ **Encoder-Only** êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë‹¤ìŒ ì„¹ì…˜ì—ì„œ ë°°ìš¸ [Vision Transformer (ViT)](./03-vit.md)ê°€ ë°”ë¡œ Encoder-Only Transformerì…ë‹ˆë‹¤.

## ì‹¤ìŠµ: ì§ì ‘ í•´ë³´ê¸°

### Positional Encoding êµ¬í˜„ê³¼ ì‹œê°í™”

```python
import torch
import torch.nn as nn
import math
import matplotlib.pyplot as plt

class PositionalEncoding(nn.Module):
    """
    ì‚¬ì¸/ì½”ì‚¬ì¸ ê¸°ë°˜ Positional Encoding
    ì‹œí€€ìŠ¤ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤
    """
    def __init__(self, d_model, max_len=5000):
        super().__init__()

        # (max_len, d_model) í¬ê¸°ì˜ PE í…Œì´ë¸” ë¯¸ë¦¬ ê³„ì‚°
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)

        # ì£¼íŒŒìˆ˜ ê³„ì‚°: 10000^(2i/d_model)
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)
        )

        # ì§ìˆ˜ ì°¨ì›ì€ sin, í™€ìˆ˜ ì°¨ì›ì€ cos
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€: (1, max_len, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        # ì…ë ¥ì— ìœ„ì¹˜ ì •ë³´ ë”í•˜ê¸°
        return x + self.pe[:, :x.size(1)]

# PE ì‹œê°í™”
d_model = 128
pe_layer = PositionalEncoding(d_model)
pe_values = pe_layer.pe[0, :100, :].numpy()  # ì²˜ìŒ 100ê°œ ìœ„ì¹˜

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# ì „ì²´ PE íˆíŠ¸ë§µ
im = axes[0].imshow(pe_values.T, cmap='RdBu', aspect='auto')
axes[0].set_xlabel('ìœ„ì¹˜ (Position)')
axes[0].set_ylabel('ì°¨ì› (Dimension)')
axes[0].set_title('Positional Encoding íˆíŠ¸ë§µ')
plt.colorbar(im, ax=axes[0])

# íŠ¹ì • ì°¨ì›ì˜ íŒŒí˜• ë¹„êµ
for dim in [0, 10, 50, 100]:
    axes[1].plot(pe_values[:, dim], label=f'dim {dim}')
axes[1].set_xlabel('ìœ„ì¹˜ (Position)')
axes[1].set_ylabel('ê°’')
axes[1].set_title('ì°¨ì›ë³„ PE íŒŒí˜• (ì£¼íŒŒìˆ˜ ì°¨ì´)')
axes[1].legend()

plt.tight_layout()
plt.show()
```

### Transformer Encoder ë¸”ë¡ êµ¬í˜„

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class TransformerEncoderBlock(nn.Module):
    """
    Transformer Encoder í•œ ë¸”ë¡ (Pre-Norm ë°©ì‹)
    = LayerNorm â†’ Multi-Head Attention â†’ ì”ì°¨ì—°ê²°
    â†’ LayerNorm â†’ FFN â†’ ì”ì°¨ì—°ê²°
    """
    def __init__(self, d_model=512, num_heads=8, d_ff=2048, dropout=0.1):
        super().__init__()

        # Multi-Head Attention
        self.attention = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True  # (batch, seq, dim) í˜•íƒœ ì‚¬ìš©
        )

        # Feed-Forward Network
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),     # ì°¨ì› í™•ì¥ (512 â†’ 2048)
            nn.GELU(),                     # í™œì„±í™” í•¨ìˆ˜ (í˜„ëŒ€ì  ì„ íƒ)
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),      # ì°¨ì› ë³µì› (2048 â†’ 512)
            nn.Dropout(dropout),
        )

        # Layer Normalization (Pre-Norm ë°©ì‹)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, x, mask=None):
        # Pre-Norm + Self-Attention + Residual
        normed = self.norm1(x)
        attn_output, attn_weights = self.attention(normed, normed, normed, attn_mask=mask)
        x = x + attn_output  # ì”ì°¨ ì—°ê²°

        # Pre-Norm + FFN + Residual
        normed = self.norm2(x)
        ffn_output = self.ffn(normed)
        x = x + ffn_output  # ì”ì°¨ ì—°ê²°

        return x, attn_weights

# í…ŒìŠ¤íŠ¸: ì¸ì½”ë” ë¸”ë¡ í•˜ë‚˜ í†µê³¼
d_model = 512
batch_size = 2
seq_len = 16  # ì˜ˆ: 4x4 ì´ë¯¸ì§€ íŒ¨ì¹˜

encoder_block = TransformerEncoderBlock(d_model=d_model, num_heads=8)
x = torch.randn(batch_size, seq_len, d_model)

output, weights = encoder_block(x)
print(f"ì…ë ¥ í¬ê¸°: {x.shape}")       # [2, 16, 512]
print(f"ì¶œë ¥ í¬ê¸°: {output.shape}")   # [2, 16, 512] â€” ì…ë ¥ê³¼ ë™ì¼!
print(f"ì–´í…ì…˜ ë§µ: {weights.shape}")  # [2, 16, 16]
```

### Nê°œ ë ˆì´ì–´ë¥¼ ìŒ“ì€ ì™„ì „í•œ Encoder

```python
class TransformerEncoder(nn.Module):
    """
    Nê°œì˜ Encoder ë¸”ë¡ì„ ìŒ“ì€ ì™„ì „í•œ Transformer Encoder
    """
    def __init__(self, d_model=512, num_heads=8, d_ff=2048,
                 num_layers=6, max_len=5000, dropout=0.1):
        super().__init__()

        # Positional Encoding
        self.pos_encoding = PositionalEncoding(d_model, max_len)
        self.dropout = nn.Dropout(dropout)

        # Nê°œì˜ Encoder ë¸”ë¡
        self.layers = nn.ModuleList([
            TransformerEncoderBlock(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])

        # ìµœì¢… Layer Norm (Pre-Norm ë°©ì‹ì—ì„œ í•„ìš”)
        self.final_norm = nn.LayerNorm(d_model)

    def forward(self, x, mask=None):
        # ìœ„ì¹˜ ì •ë³´ ì£¼ì…
        x = self.pos_encoding(x)
        x = self.dropout(x)

        # Nê°œ ë ˆì´ì–´ ìˆœì°¨ í†µê³¼
        all_weights = []
        for layer in self.layers:
            x, weights = layer(x, mask)
            all_weights.append(weights)

        # ìµœì¢… ì •ê·œí™”
        x = self.final_norm(x)

        return x, all_weights

# ì™„ì „í•œ Encoder í…ŒìŠ¤íŠ¸
encoder = TransformerEncoder(
    d_model=256,
    num_heads=8,
    d_ff=1024,
    num_layers=6,
    dropout=0.1
)

x = torch.randn(2, 16, 256)  # ë°°ì¹˜ 2, íŒ¨ì¹˜ 16ê°œ, 256ì°¨ì›
output, all_weights = encoder(x)

print(f"ì…ë ¥: {x.shape}")                  # [2, 16, 256]
print(f"ì¶œë ¥: {output.shape}")              # [2, 16, 256]
print(f"ë ˆì´ì–´ ìˆ˜: {len(all_weights)}")      # 6
print(f"ì´ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in encoder.parameters()):,}")
```

## ë” ê¹Šì´ ì•Œì•„ë³´ê¸°

### Transformer ì´ë¦„ì˜ ìœ ë˜

"Transformer"ë¼ëŠ” ì´ë¦„ì€ ë…¼ë¬¸ ì €ìë“¤ì´ ì´ ëª¨ë¸ì´ ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì¶œë ¥ ì‹œí€€ìŠ¤ë¡œ **ë³€í™˜(transform)**í•œë‹¤ëŠ” ì˜ë¯¸ì—ì„œ ë¶™ì˜€ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¬ë¯¸ìˆëŠ” ë’·ì´ì•¼ê¸°ê°€ ìˆëŠ”ë°ìš”, ë…¼ë¬¸ ì œì¶œ ë‹¹ì‹œ "Attention Is All You Need"ë¼ëŠ” ë„ë°œì ì¸ ì œëª©ì´ ë¦¬ë·°ì–´ë“¤ ì‚¬ì´ì—ì„œ ë…¼ë€ì´ ë˜ì—ˆë‹¤ê³  í•©ë‹ˆë‹¤. "ê¸°ì¡´ ì—°êµ¬ë¥¼ ë„ˆë¬´ ë¬´ì‹œí•˜ëŠ” ê²ƒ ì•„ë‹ˆëƒ"ëŠ” ì˜ê²¬ì´ ìˆì—ˆì§€ë§Œ, ê²°ê³¼ì ìœ¼ë¡œ ì´ ëŒ€ë‹´í•œ ì œëª©ì´ ì˜ˆì–¸ì²˜ëŸ¼ ë§ì•„ë–¨ì–´ì¡Œì£ .

ë” í¥ë¯¸ë¡œìš´ ê²ƒì€, ë…¼ë¬¸ì˜ 8ëª… ê³µì €ì ì¤‘ ëŒ€ë¶€ë¶„ì´ ì´í›„ Googleì„ ë– ë‚˜ ê°ì AI ìŠ¤íƒ€íŠ¸ì—…ì„ ì°½ì—…í–ˆë‹¤ëŠ” ì‚¬ì‹¤ì…ë‹ˆë‹¤. Noam ShazeerëŠ” Character.AIë¥¼, Aidan GomezëŠ” Cohereë¥¼, Llion JonesëŠ” Sakana AIë¥¼ ë§Œë“¤ì—ˆê³ , ì´ë“¤ì˜ ìŠ¤íƒ€íŠ¸ì—…ì€ ëª¨ë‘ ìˆ˜ì¡° ì›ëŒ€ ê°€ì¹˜ë¡œ í‰ê°€ë°›ê³  ìˆìŠµë‹ˆë‹¤.

### Cross-Attention: ë‘ ì„¸ê³„ë¥¼ ì‡ëŠ” ë‹¤ë¦¬

ë””ì½”ë”ì—ë§Œ ìˆëŠ” íŠ¹ë³„í•œ ë ˆì´ì–´ê°€ **Cross-Attention**ì…ë‹ˆë‹¤. Self-Attentionê³¼ì˜ ì°¨ì´ëŠ” ê°„ë‹¨í•©ë‹ˆë‹¤:

- **Self-Attention**: Q, K, V ëª¨ë‘ ê°™ì€ ì¶œì²˜ â†’ ìê¸° ìì‹ ì˜ ê´€ê³„ íŒŒì•…
- **Cross-Attention**: QëŠ” ë””ì½”ë”ì—ì„œ, Kì™€ VëŠ” ì¸ì½”ë”ì—ì„œ â†’ ë‘ ì‹œí€€ìŠ¤ ê°„ ê´€ê³„ íŒŒì•…

ì´ íŒ¨í„´ì€ ì»´í“¨í„° ë¹„ì „ì—ì„œë„ ì¤‘ìš”í•˜ê²Œ ì“°ì…ë‹ˆë‹¤. [DETR](../07-object-detection/05-detr.md)ì—ì„œ Object Queryê°€ ì´ë¯¸ì§€ íŠ¹ì§•ê³¼ êµì°¨ ì–´í…ì…˜í•˜ì—¬ ê°ì²´ë¥¼ íƒì§€í•˜ê³ , [Mask2Former](../08-segmentation/03-panoptic-segmentation.md)ì—ì„œ ë§ˆìŠ¤í¬ ì¿¼ë¦¬ê°€ íŠ¹ì§• ë§µê³¼ êµì°¨ ì–´í…ì…˜í•˜ì—¬ ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ìˆ˜í–‰í•˜ë©°, [SAM](../08-segmentation/04-sam.md)ì—ì„œ í”„ë¡¬í”„íŠ¸ ì„ë² ë”©ì´ ì´ë¯¸ì§€ ì„ë² ë”©ê³¼ êµì°¨ ì–´í…ì…˜í•˜ì—¬ ë§ˆìŠ¤í¬ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. Stable Diffusionì˜ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ìƒì„±ì—ì„œë„ Cross-Attentionì´ í•µì‹¬ ì—­í• ì„ í•˜ì£ .

## í”í•œ ì˜¤í•´ì™€ íŒ

> âš ï¸ **í”í•œ ì˜¤í•´**: "TransformerëŠ” ê·¸ëƒ¥ ì–´í…ì…˜ ë©ì–´ë¦¬ë‹¤"ë¼ê³  ìƒê°í•˜ê¸° ì‰½ì§€ë§Œ, FFNì´ ì „ì²´ íŒŒë¼ë¯¸í„°ì˜ ì•½ **2/3**ë¥¼ ì°¨ì§€í•©ë‹ˆë‹¤. ì–´í…ì…˜ì€ "ì–´ë””ë¥¼ ë³¼ì§€" ê²°ì •í•˜ê³ , FFNì´ "ë¬´ì—‡ì„ í• ì§€" ê²°ì •í•˜ëŠ” ë™ë“±í•˜ê²Œ ì¤‘ìš”í•œ êµ¬ì„± ìš”ì†Œì…ë‹ˆë‹¤.

> ğŸ’¡ **ì•Œê³  ê³„ì…¨ë‚˜ìš”?**: Positional Encodingì˜ sin/cos ë°©ì‹ì€ ì›ë˜ ë…¼ë¬¸ì˜ ì„ íƒì¼ ë¿, ìœ ì¼í•œ ë°©ë²•ì€ ì•„ë‹™ë‹ˆë‹¤. ìµœê·¼ì—ëŠ” **í•™ìŠµ ê°€ëŠ¥í•œ ìœ„ì¹˜ ì„ë² ë”©(Learnable PE)**ì´ë‚˜ **Rotary Position Embedding(RoPE)**ì´ ë” ë„ë¦¬ ì“°ì…ë‹ˆë‹¤. ViTëŠ” í•™ìŠµ ê°€ëŠ¥í•œ PEë¥¼, LLaMAëŠ” RoPEë¥¼ ì‚¬ìš©í•˜ì£ .

> ğŸ”¥ **ì‹¤ë¬´ íŒ**: Pre-Normê³¼ Post-Norm ì¤‘ ê³ ë¯¼ëœë‹¤ë©´, ë¬´ì¡°ê±´ **Pre-Norm**ì„ ì„ íƒí•˜ì„¸ìš”. 2024ë…„ ê¸°ì¤€ GPT-3, LLaMA, PaLM, Falcon, Mistral ë“± ê±°ì˜ ëª¨ë“  ëŒ€ê·œëª¨ ëª¨ë¸ì´ Pre-Normì„ ì±„íƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. í•™ìŠµ ì•ˆì •ì„±ì´ ì••ë„ì ìœ¼ë¡œ ì¢‹ìŠµë‹ˆë‹¤.

> âš ï¸ **í”í•œ ì˜¤í•´**: "ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ í•­ìƒ í•¨ê»˜ ì¨ì•¼ í•œë‹¤"ê³  ìƒê°í•  ìˆ˜ ìˆì§€ë§Œ, í˜„ëŒ€ ë¹„ì „ ëª¨ë¸ì˜ ëŒ€ë¶€ë¶„ì€ **Encoder-Only** êµ¬ì¡°ì…ë‹ˆë‹¤. ViT, Swin Transformer, DeiT ë“±ì´ ëª¨ë‘ Encoderë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

## í•µì‹¬ ì •ë¦¬

| ê°œë… | ì„¤ëª… |
|------|------|
| Encoder | ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ê³ ì°¨ì› í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë“ˆ |
| Decoder | ì¸ì½”ë” ì¶œë ¥ì„ ì°¸ì¡°í•˜ì—¬ ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë“ˆ |
| Positional Encoding | ìˆœì„œ ì •ë³´ê°€ ì—†ëŠ” Transformerì— ìœ„ì¹˜ ì •ë³´ë¥¼ ì£¼ì…í•˜ëŠ” ë°©ë²• |
| Feed-Forward Network | ì–´í…ì…˜ì´ ëª¨ì€ ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ëŠ” 2ì¸µ MLP, íŒŒë¼ë¯¸í„°ì˜ 2/3 ì°¨ì§€ |
| Residual Connection | $x + \text{SubLayer}(x)$ â€” ê¸°ìš¸ê¸° íë¦„ì„ ìœ„í•œ ì§€ë¦„ê¸¸ |
| Layer Normalization | íŠ¹ì„± ì°¨ì› ê¸°ì¤€ ì •ê·œí™”, í•™ìŠµ ì•ˆì •ì„± í™•ë³´ |
| Pre-Norm | í˜„ëŒ€ í‘œì¤€ â€” ì„œë¸Œë ˆì´ì–´ **ì´ì „ì—** ì •ê·œí™” ì ìš© |
| Cross-Attention | QëŠ” ë””ì½”ë”, K/VëŠ” ì¸ì½”ë” â€” ë‘ ì‹œí€€ìŠ¤ ê°„ ì—°ê²° ë‹¤ë¦¬ |
| Encoder-Only | ViT, BERT â€” ë¹„ì „ê³¼ ë¶„ë¥˜ì— ì£¼ë¡œ ì‚¬ìš© |

## ë‹¤ìŒ ì„¹ì…˜ ë¯¸ë¦¬ë³´ê¸°

Transformerì˜ ì „ì²´ êµ¬ì¡°ë¥¼ ì´í•´í–ˆìœ¼ë‹ˆ, ì´ì œ ë“œë””ì–´ ì´ê²ƒì„ **ì´ë¯¸ì§€ì— ì ìš©**í•  ì°¨ë¡€ì…ë‹ˆë‹¤! ë‹¤ìŒ [Vision Transformer (ViT)](./03-vit.md)ì—ì„œëŠ” ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ë¡œ ì˜ë¼ Transformerì— ë„£ëŠ” ê¸°ë°œí•œ ì•„ì´ë””ì–´ê°€ ì–´ë–»ê²Œ CNNì˜ ì™•ì¢Œë¥¼ ìœ„í˜‘í–ˆëŠ”ì§€ ì•Œì•„ë´…ë‹ˆë‹¤.

## ì°¸ê³  ìë£Œ

- [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762) - Transformer ì›ë³¸ ë…¼ë¬¸
- [The Illustrated Transformer (Jay Alammar)](https://jalammar.github.io/illustrated-transformer/) - ê°€ì¥ ìœ ëª…í•œ ì‹œê°ì  Transformer í•´ì„¤
- [Transformer Architecture: The Positional Encoding (Amirhossein Kazemnejad)](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/) - Positional Encodingì˜ ìˆ˜í•™ì  ì§ê´€ ì„¤ëª…
- [Why Pre-Norm Became the Default in Transformers (2024)](https://medium.com/@ashutoshs81127/why-pre-norm-became-the-default-in-transformers-4229047e2620) - Pre-Norm vs Post-Norm ë¹„êµ
- [Transformer Explainer (Georgia Tech)](https://poloclub.github.io/transformer-explainer/) - ì¸í„°ë™í‹°ë¸Œ Transformer ì‹œê°í™” ë„êµ¬
