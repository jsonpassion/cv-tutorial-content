# ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €

> Cross-Entropy, MSE, Adam, SGD

## ê°œìš”

ì—­ì „íŒŒì—ì„œ "ì†ì‹¤ì„ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•œë‹¤"ê³  ë°°ì› ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ë‘ ê°€ì§€ ì§ˆë¬¸ì´ ë‚¨ìŠµë‹ˆë‹¤: **"ì†ì‹¤"ì„ ì–´ë–»ê²Œ ì¸¡ì •í•˜ëŠ”ê°€?** (ì†ì‹¤ í•¨ìˆ˜) ê·¸ë¦¬ê³  **"ì¡°ì •"ì„ êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–»ê²Œ í•˜ëŠ”ê°€?** (ì˜µí‹°ë§ˆì´ì €) ì´ ì„¹ì…˜ì—ì„œ ë‘˜ ë‹¤ ë‹¤ë£¹ë‹ˆë‹¤.

**ì„ ìˆ˜ ì§€ì‹**: [ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜](./03-backpropagation.md) â€” ê²½ì‚¬í•˜ê°•ë²•, ê¸°ìš¸ê¸°
**í•™ìŠµ ëª©í‘œ**:
- ë¶„ë¥˜ì™€ íšŒê·€ì— ì í•©í•œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì„ íƒí•  ìˆ˜ ìˆë‹¤
- SGD, Adam ë“± ì˜µí‹°ë§ˆì´ì €ì˜ ì°¨ì´ë¥¼ ì´í•´í•œë‹¤
- PyTorchì—ì„œ ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì¡°í•©í•˜ì—¬ í•™ìŠµí•  ìˆ˜ ìˆë‹¤

## ì™œ ì•Œì•„ì•¼ í• ê¹Œ?

ì˜ëª»ëœ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì“°ë©´ ëª¨ë¸ì´ ì—‰ëš±í•œ ê²ƒì„ ìµœì í™”í•©ë‹ˆë‹¤. ë¶€ì ì ˆí•œ ì˜µí‹°ë§ˆì´ì €ëŠ” í•™ìŠµì´ ëŠë¦¬ê±°ë‚˜ ë¶ˆì•ˆì •í•´ì§‘ë‹ˆë‹¤. **Cross-Entropy + Adam**ì€ í˜„ëŒ€ ë”¥ëŸ¬ë‹ì˜ í‘œì¤€ ì¡°í•©ì´ì§€ë§Œ, ìƒí™©ì— ë”°ë¼ ë‹¤ë¥¸ ì„ íƒì´ í•„ìš”í•  ë•Œë¥¼ ì•„ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

## í•µì‹¬ ê°œë…

### 1. ì†ì‹¤ í•¨ìˆ˜ â€” ì–¼ë§ˆë‚˜ í‹€ë ¸ëŠ”ì§€ ì¸¡ì •

> ğŸ’¡ **ë¹„ìœ **: ì†ì‹¤ í•¨ìˆ˜ëŠ” **ì±„ì í‘œ**ì…ë‹ˆë‹¤. í•™ìƒ(ëª¨ë¸)ì˜ ë‹µì•ˆ(ì˜ˆì¸¡)ì„ ì •ë‹µê³¼ ë¹„êµí•´ ì ìˆ˜ë¥¼ ë§¤ê¹ë‹ˆë‹¤. ë‹¤ë§Œ ì—¬ê¸°ì„œ ì ìˆ˜ê°€ **ë‚®ì„ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤** â€” í‹€ë¦° ì •ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ê²ƒì´ë‹ˆê¹Œìš”.

### 2. MSE (Mean Squared Error) â€” íšŒê·€ ë¬¸ì œì˜ ê¸°ë³¸

> ğŸ’¡ **ë¹„ìœ **: ë‹¤íŠ¸ë¥¼ ë˜ì§ˆ ë•Œ **ê³¼ë… ì¤‘ì‹¬ì—ì„œ ì–¼ë§ˆë‚˜ ë²—ì–´ë‚¬ëŠ”ì§€ë¥¼ ì œê³±**í•´ì„œ ì¸¡ì •í•©ë‹ˆë‹¤. ë§ì´ ë²—ì–´ë‚˜ë©´ ë²Œì ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì»¤ì§‘ë‹ˆë‹¤.

> **MSE = (1/N) Ã— Î£(ì˜ˆì¸¡ê°’ âˆ’ ì •ë‹µ)Â²**

```python
import torch
import torch.nn as nn

criterion = nn.MSELoss()

prediction = torch.tensor([2.5, 3.0, 4.5])
target = torch.tensor([3.0, 3.0, 5.0])

loss = criterion(prediction, target)
print(f"ì˜ˆì¸¡: {prediction.tolist()}")
print(f"ì •ë‹µ: {target.tolist()}")
print(f"MSE ì†ì‹¤: {loss.item():.4f}")
# (0.25 + 0.0 + 0.25) / 3 = 0.1667
```

| íŠ¹ì„± | ë‚´ìš© |
|------|------|
| ìš©ë„ | **íšŒê·€** (ì—°ì† ê°’ ì˜ˆì¸¡) |
| ë²”ìœ„ | 0 ~ âˆ (0ì´ ìµœì ) |
| íŠ¹ì§• | í° ì˜¤ì°¨ì— í° ë²Œì  (ì œê³± íš¨ê³¼) |
| ì˜ˆì‹œ | ì§‘ê°’ ì˜ˆì¸¡, ì˜¨ë„ ì˜ˆì¸¡, ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ |

### 3. Cross-Entropy â€” ë¶„ë¥˜ ë¬¸ì œì˜ í‘œì¤€

> ğŸ’¡ **ë¹„ìœ **: "ì´ ì‚¬ì§„ì€ ê³ ì–‘ì´ì¼ê¹Œ, ê°•ì•„ì§€ì¼ê¹Œ?" ëª¨ë¸ì´ "ê³ ì–‘ì´ 90%, ê°•ì•„ì§€ 10%"ë¼ê³  ë‹µí–ˆëŠ”ë° ì •ë‹µì´ ê³ ì–‘ì´ë¼ë©´ ë²Œì ì´ ì ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ "ê³ ì–‘ì´ 10%, ê°•ì•„ì§€ 90%"ë¼ê³  í™•ì‹ ìˆê²Œ í‹€ë¦¬ë©´ ë²Œì ì´ **ë§¤ìš° í¬ê²Œ** ë¶€ê³¼ë©ë‹ˆë‹¤.

```python
import torch
import torch.nn as nn

# ë¶„ë¥˜: 3ê°œ í´ë˜ìŠ¤ (ê³ ì–‘ì´, ê°•ì•„ì§€, ìƒˆ)
criterion = nn.CrossEntropyLoss()

# ëª¨ë¸ ì¶œë ¥ (logits, softmax ì „ ê°’)
logits = torch.tensor([[2.0, 1.0, 0.1]])  # í´ë˜ìŠ¤0(ê³ ì–‘ì´)ì— ë†’ì€ ì ìˆ˜
target = torch.tensor([0])                 # ì •ë‹µ: í´ë˜ìŠ¤0(ê³ ì–‘ì´)

loss = criterion(logits, target)
print(f"ëª¨ë¸ ì¶œë ¥(logits): {logits.tolist()}")
print(f"ì •ë‹µ í´ë˜ìŠ¤: {target.item()}")
print(f"Cross-Entropy ì†ì‹¤: {loss.item():.4f}")

# í™•ë¥ ë¡œ ë³€í™˜í•´ì„œ í™•ì¸
probs = torch.softmax(logits, dim=1)
print(f"í™•ë¥ : {[f'{p:.2%}' for p in probs[0].tolist()]}")
```

| íŠ¹ì„± | ë‚´ìš© |
|------|------|
| ìš©ë„ | **ë¶„ë¥˜** (í´ë˜ìŠ¤ ì˜ˆì¸¡) |
| ì…ë ¥ | Logits (softmax ì „ ê°’) â€” PyTorchê°€ ë‚´ë¶€ì—ì„œ softmax ì ìš© |
| íŠ¹ì§• | í™•ì‹ ìˆê²Œ í‹€ë¦´ìˆ˜ë¡ í° ë²Œì  |
| ì˜ˆì‹œ | ì´ë¯¸ì§€ ë¶„ë¥˜, ê°ì • ë¶„ì„, ê°ì²´ íƒì§€ í´ë˜ìŠ¤ |

### 4. ì´ì§„ ë¶„ë¥˜ìš© â€” BCELoss

```python
import torch
import torch.nn as nn

# ì´ì§„ ë¶„ë¥˜: ìŠ¤íŒ¸(1) vs ì •ìƒ(0)
criterion = nn.BCEWithLogitsLoss()  # Sigmoid + BCEë¥¼ í•©ì¹œ ë²„ì „

logits = torch.tensor([2.0, -1.0, 0.5])   # ëª¨ë¸ ì¶œë ¥
target = torch.tensor([1.0, 0.0, 1.0])     # ì •ë‹µ

loss = criterion(logits, target)
print(f"BCE ì†ì‹¤: {loss.item():.4f}")
```

### 5. ì†ì‹¤ í•¨ìˆ˜ ì„ íƒ ê°€ì´ë“œ

| ë¬¸ì œ ìœ í˜• | ì†ì‹¤ í•¨ìˆ˜ | PyTorch í´ë˜ìŠ¤ |
|----------|----------|---------------|
| íšŒê·€ (ì—°ì† ê°’) | MSE | `nn.MSELoss()` |
| íšŒê·€ (ì´ìƒì¹˜ì— ê°•ê±´) | MAE / Huber | `nn.L1Loss()` / `nn.SmoothL1Loss()` |
| ë‹¤ì¤‘ ë¶„ë¥˜ | Cross-Entropy | `nn.CrossEntropyLoss()` |
| ì´ì§„ ë¶„ë¥˜ | Binary CE | `nn.BCEWithLogitsLoss()` |
| ì„¸ê·¸ë©˜í…Œì´ì…˜ | Dice + CE | ì»¤ìŠ¤í…€ ì¡°í•© |

---

### 6. ì˜µí‹°ë§ˆì´ì € â€” ì–´ë–»ê²Œ ì—…ë°ì´íŠ¸í•  ê²ƒì¸ê°€

> ğŸ’¡ **ë¹„ìœ **: ê²½ì‚¬í•˜ê°•ë²•ì´ "ë‚´ë ¤ê°€ëŠ” ë°©í–¥ìœ¼ë¡œ ê±¸ì–´ë¼"ë¼ë©´, ì˜µí‹°ë§ˆì´ì €ëŠ” **ê±·ëŠ” ì „ëµ**ì…ë‹ˆë‹¤. SGDëŠ” ìš°ì§í•˜ê²Œ í•œ ë°©í–¥, Adamì€ ì§€í˜•ì„ ê¸°ì–µí•˜ë©° ì˜ë¦¬í•˜ê²Œ ê±·ìŠµë‹ˆë‹¤.

### 7. SGD (Stochastic Gradient Descent)

ê°€ì¥ ê¸°ë³¸ì ì¸ ì˜µí‹°ë§ˆì´ì €. ê¸°ìš¸ê¸° ë°©í–¥ìœ¼ë¡œ í•™ìŠµë¥ ë§Œí¼ ì´ë™í•©ë‹ˆë‹¤.

```python
import torch.optim as optim

optimizer = optim.SGD(model.parameters(), lr=0.01)

# ëª¨ë©˜í…€ ì¶”ê°€ (ê´€ì„±ì„ ì£¼ì–´ ì§„ë™ ì¤„ì„)
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
```

### 8. Adam â€” í˜„ëŒ€ì˜ ê¸°ë³¸ê°’

Adam = **Ada**ptive **M**oment Estimation. ê° íŒŒë¼ë¯¸í„°ë§ˆë‹¤ í•™ìŠµë¥ ì„ ìë™ ì¡°ì ˆí•©ë‹ˆë‹¤.

> ğŸ’¡ **ë¹„ìœ **: SGDê°€ ëª¨ë“  ê¸¸ì„ ê°™ì€ ë³´í­ìœ¼ë¡œ ê±·ëŠ”ë‹¤ë©´, Adamì€ **í‰íƒ„í•œ ê¸¸ì—ì„œëŠ” ë¹ ë¥´ê²Œ, ê°€íŒŒë¥¸ ê¸¸ì—ì„œëŠ” ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ** ê±¸ìŒí­ì„ ìë™ ì¡°ì ˆí•©ë‹ˆë‹¤.

```python
import torch.optim as optim

# Adam (ê°€ì¥ ë§ì´ ì‚¬ìš©)
optimizer = optim.Adam(model.parameters(), lr=0.001)

# AdamW (ê°€ì¤‘ì¹˜ ê°ì‡  ê°œì„  ë²„ì „, Transformerì—ì„œ í‘œì¤€)
optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
```

### 9. ì˜µí‹°ë§ˆì´ì € ë¹„êµ

| ì˜µí‹°ë§ˆì´ì € | í•™ìŠµë¥  ì¡°ì ˆ | ëª¨ë©˜í…€ | ì¥ì  | ë‹¨ì  |
|-----------|-----------|--------|------|------|
| **SGD** | ê³ ì • | ì„ íƒ | ì¼ë°˜í™” ì„±ëŠ¥ ì¢‹ìŒ | ìˆ˜ë ´ ëŠë¦¼, íŠœë‹ ì–´ë ¤ì›€ |
| **SGD+Momentum** | ê³ ì • | ìˆìŒ | ì§„ë™ ê°ì†Œ, ë” ë¹ ë¦„ | í•™ìŠµë¥  ìˆ˜ë™ ì„¤ì • |
| **Adam** | íŒŒë¼ë¯¸í„°ë³„ ìë™ | ìˆìŒ | ë¹ ë¥¸ ìˆ˜ë ´, íŠœë‹ ì‰¬ì›€ | ê°€ë” ì¼ë°˜í™” ì•½í•¨ |
| **AdamW** | íŒŒë¼ë¯¸í„°ë³„ ìë™ | ìˆìŒ | Adam + ì˜¬ë°”ë¥¸ ê°€ì¤‘ì¹˜ ê°ì‡  | **Transformer í‘œì¤€** |

> **ì‹¤ë¬´ ê°€ì´ë“œ**: ì²˜ìŒì—ëŠ” **Adam(lr=0.001)**ë¡œ ì‹œì‘, ë¯¸ì„¸ ì¡°ì •ì´ í•„ìš”í•˜ë©´ **AdamW** ë˜ëŠ” **SGD+Momentum**ì„ ì‹œë„í•˜ì„¸ìš”.

## ì‹¤ìŠµ: ì§ì ‘ í•´ë³´ê¸°

### ì˜µí‹°ë§ˆì´ì €ë³„ í•™ìŠµ ì†ë„ ë¹„êµ

```python
import torch
import torch.nn as nn

def train_with_optimizer(opt_name, opt_class, lr):
    """ë™ì¼í•œ ëª¨ë¸ì„ ë‹¤ë¥¸ ì˜µí‹°ë§ˆì´ì €ë¡œ í•™ìŠµí•˜ê³  ì†ì‹¤ ë³€í™” ê´€ì°°"""
    torch.manual_seed(42)

    model = nn.Sequential(nn.Linear(1, 16), nn.ReLU(), nn.Linear(16, 1))
    criterion = nn.MSELoss()

    if opt_name == "SGD+Momentum":
        optimizer = opt_class(model.parameters(), lr=lr, momentum=0.9)
    else:
        optimizer = opt_class(model.parameters(), lr=lr)

    x = torch.linspace(-3, 3, 50).unsqueeze(1)
    y = torch.sin(x)

    losses = []
    for epoch in range(200):
        pred = model(x)
        loss = criterion(pred, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        if epoch % 50 == 0:
            losses.append(loss.item())

    print(f"{opt_name:15s} | ì†ì‹¤ ë³€í™”: {' â†’ '.join(f'{l:.4f}' for l in losses)}")

train_with_optimizer("SGD", torch.optim.SGD, 0.01)
train_with_optimizer("SGD+Momentum", torch.optim.SGD, 0.01)
train_with_optimizer("Adam", torch.optim.Adam, 0.001)
train_with_optimizer("AdamW", torch.optim.AdamW, 0.001)
```

## í•µì‹¬ ì •ë¦¬

| ê°œë… | ì„¤ëª… |
|------|------|
| **MSE** | íšŒê·€ìš© ì†ì‹¤. ì˜¤ì°¨ì˜ ì œê³± í‰ê·  |
| **Cross-Entropy** | ë¶„ë¥˜ìš© ì†ì‹¤. í™•ì‹ ìˆê²Œ í‹€ë¦¬ë©´ í° ë²Œì  |
| **SGD** | ê¸°ë³¸ ê²½ì‚¬í•˜ê°•ë²•. ë‹¨ìˆœí•˜ì§€ë§Œ ì¼ë°˜í™” ì¢‹ìŒ |
| **Adam** | ì ì‘ì  í•™ìŠµë¥ . ë¹ ë¥¸ ìˆ˜ë ´, ê°€ì¥ ë„ë¦¬ ì‚¬ìš© |
| **AdamW** | Adam + ê°€ì¤‘ì¹˜ ê°ì‡  ê°œì„ . Transformerì˜ í‘œì¤€ |
| **í•™ìŠµë¥ ** | ê°€ì¥ ì¤‘ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°. ë³´í†µ 0.001ì—ì„œ ì‹œì‘ |

## ë‹¤ìŒ ì„¹ì…˜ ë¯¸ë¦¬ë³´ê¸°

ì‹ ê²½ë§ì˜ ì´ë¡ ì  ê¸°ì´ˆë¥¼ ëª¨ë‘ ë‹¤ë¤˜ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì„¹ì…˜ **[PyTorch ê¸°ì´ˆ](./05-pytorch-fundamentals.md)**ì—ì„œëŠ” ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ê°œë…ë“¤ì„ PyTorchë¡œ ì‹¤ì „ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤. í…ì„œ ì—°ì‚°, Dataset, DataLoader, í•™ìŠµ ë£¨í”„ê¹Œì§€ ì‹¤ë¬´ì— í•„ìš”í•œ ëª¨ë“  ê²ƒì„ ë‹¤ë£¹ë‹ˆë‹¤.

## ì°¸ê³  ìë£Œ

- [DataCamp - Cross-Entropy Loss Function](https://www.datacamp.com/tutorial/the-cross-entropy-loss-function-in-machine-learning) - Cross-Entropyì˜ ìˆ˜í•™ì  ë°°ê²½ê³¼ ì‹¤ìš©ì  ì‚¬ìš©ë²•
- [Machine Learning Mastery - How to Choose Loss Functions](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/) - ë¬¸ì œ ìœ í˜•ë³„ ì†ì‹¤ í•¨ìˆ˜ ì„ íƒ ê°€ì´ë“œ
- [Medium - Demystifying Loss Functions & Adam Optimizer](https://medium.com/@akankshasinha247/how-ai-learns-demystifying-loss-functions-the-adam-optimizer-ed29862e389c) - ì†ì‹¤ í•¨ìˆ˜ì™€ Adamì˜ ì§ê´€ì  ì„¤ëª…
- [arXiv - Loss Functions and Metrics in Deep Learning](https://arxiv.org/html/2307.02694v5) - ì†ì‹¤ í•¨ìˆ˜ ì¢…í•© ì„œë² ì´ ë…¼ë¬¸
